# -*- coding: utf-8 -*-
"""Emotion Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14oZN8F8e4lJMorg_JK52fa3kJnrLp3q6
"""

!pip install deepface
!pip install transformers
!pip install gradio
from deepface import DeepFace
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import gradio as gr
import torch
import os

# Load pre-trained GPT-2 for caption generation
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Emotion detection function using DeepFace
def detect_emotion(image):
    # DeepFace detects emotions from the image
    emotion_result = DeepFace.analyze(image, actions=['emotion'])
    dominant_emotion = emotion_result[0]['dominant_emotion']
    return dominant_emotion

# Caption generation function
def generate_caption(emotion):
    # Generate a caption related to the detected emotion using GPT-2
    prompt = f"The person in the image looks {emotion}."
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)
    caption = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return caption

# Combined function for Gradio interface
def process_image(image):
    # Step 1: Detect emotion
    emotion = detect_emotion(image)

    # Step 2: Generate a caption based on the emotion
    caption = generate_caption(emotion)

    return f"Emotion detected: {emotion}\nGenerated Caption: {caption}"

# Create Gradio interface
iface = gr.Interface(
    fn=process_image,
    inputs=gr.Image(),
    outputs="text",
    title="Emotion Detection & Caption Generator",
    description="Upload an image to detect the dominant emotion and generate a caption related to it."
)

# Launch the interface
iface.launch()